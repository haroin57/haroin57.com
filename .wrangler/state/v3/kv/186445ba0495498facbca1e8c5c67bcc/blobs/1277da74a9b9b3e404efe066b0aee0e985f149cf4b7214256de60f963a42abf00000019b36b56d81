{"slug":"distributed-system","name":"distributed-system","description":"Distributed system implementation for learning distributed computing","language":"Java","tags":["Distributed Computing","Microservices"],"url":"https://github.com/haroin57/distoributed-system","createdAt":"2025-12-19T13:03:37.005Z","updatedAt":"2025-12-19T13:03:37.005Z","markdown":"---\ntitle: \"distributed-system - 分散システムの学習実装\"\nsummary: \"RaftアルゴリズムとgRPC通信を使った分散システムの実装について\"\ndate: \"2025-12-18\"\nproduct: \"distributed-system\"\ntags:\n- Java\n- Distributed Computing\n- Raft\n- gRPC\n---\n\n## 目次\n\n<br />\n\n## はじめに\n\n分散システムの理論を実際に手を動かして理解するために作ったプロジェクトです。\n\nRaftコンセンサスアルゴリズムを中心に、分散合意やレプリケーションの仕組みを実装しています。「論文を読んだだけでは分からない」部分を実装を通じて理解することを目的としています。\n\n<br />\n\n## 技術スタック\n\n<br />\n\n### 言語・ランタイム\n\n| 技術 | 説明 |\n|------|------|\n| **Java 21** | Virtual Threadsを活用した並行処理 |\n| **Gradle** | ビルドツール・依存関係管理 |\n\n<br />\n\n### 通信\n\n| 技術 | 説明 |\n|------|------|\n| **gRPC** | 高速なノード間RPC通信 |\n| **Protocol Buffers** | 効率的なシリアライズフォーマット |\n\n<br />\n\n### その他\n\n| 技術 | 説明 |\n|------|------|\n| **SLF4J + Logback** | ロギング |\n| **JUnit 5** | テスト |\n\n<br />\n\n## Raftコンセンサスアルゴリズムとは\n\nRaftは分散システムにおける<strong>合意形成（コンセンサス）</strong>を実現するアルゴリズムです。\n\n複数のサーバー（ノード）が同じ状態を持つように調整し、一部のノードが故障しても全体として正しく動作し続けることを保証します。\n\n<br />\n\n### なぜRaftか\n\n以前から有名なPaxosアルゴリズムは理解が難しいことで知られています。Raftは「理解しやすさ」を設計目標に掲げ、以下の特徴を持ちます：\n\n| 特徴 | 説明 |\n|------|------|\n| **強いリーダー** | ログはリーダーからフォロワーへ一方向にのみ流れる |\n| **リーダー選出** | ランダム化タイムアウトで選出の衝突を回避 |\n| **メンバーシップ変更** | Joint Consensusによる安全な構成変更 |\n\n<br />\n\n## システムアーキテクチャ\n\n```mermaid\nflowchart TB\n    subgraph Cluster[\"Raft Cluster (3 or 5 nodes)\"]\n        Leader[\"Node A<br/>(Leader)\"]\n        Follower1[\"Node B<br/>(Follower)\"]\n        Follower2[\"Node C<br/>(Follower)\"]\n    end\n\n    subgraph ClientLayer[\"Client\"]\n        Client[\"KV操作<br/>get, put, delete\"]\n    end\n\n    Client -->|リクエスト| Leader\n    Leader -->|レプリケーション| Follower1\n    Leader -->|レプリケーション| Follower2\n    Leader -->|レスポンス| Client\n    Follower1 -.->|Heartbeat応答| Leader\n    Follower2 -.->|Heartbeat応答| Leader\n\n    style Leader fill:#134e4a,stroke:#5eead4,stroke-width:2px\n    style Client fill:#1e3a5f,stroke:#60a5fa,stroke-width:2px\n```\n\n<br />\n\n## 実装した内容\n\n<br />\n\n### 1. ノードの状態遷移\n\nRaftのノードは3つの状態を持ちます：\n\n```mermaid\nstateDiagram-v2\n    [*] --> Follower\n    Follower --> Candidate: 選挙タイムアウト\n    Candidate --> Leader: 過半数の票を獲得\n    Candidate --> Follower: 他のリーダーを発見\n    Leader --> Follower: 他のリーダーを発見\n    Candidate --> Candidate: 投票の分裂\n\n    classDef leader fill:#134e4a,stroke:#5eead4,stroke-width:2px\n    classDef candidate fill:#78350f,stroke:#fbbf24,stroke-width:2px\n    classDef follower fill:#1e3a5f,stroke:#60a5fa,stroke-width:2px\n\n    class Leader leader\n    class Candidate candidate\n    class Follower follower\n```\n\n<br />\n\n#### 実装コード\n\n```java\npublic enum NodeState {\n    FOLLOWER,\n    CANDIDATE,\n    LEADER\n}\n\npublic class RaftNode {\n    private volatile NodeState state = NodeState.FOLLOWER;\n    private volatile int currentTerm = 0;\n    private volatile String votedFor = null;\n    private final String nodeId;\n    private final List<String> peers;\n\n    // リーダーからのハートビートを受信したとき\n    public synchronized void onHeartbeatReceived(int leaderTerm) {\n        if (leaderTerm >= currentTerm) {\n            currentTerm = leaderTerm;\n            state = NodeState.FOLLOWER;\n            resetElectionTimer();\n        }\n    }\n\n    // 選挙タイムアウトが発生したとき\n    public synchronized void onElectionTimeout() {\n        if (state != NodeState.LEADER) {\n            startElection();\n        }\n    }\n}\n```\n\n<br />\n\n### 2. リーダー選出 (Leader Election)\n\n選挙プロセスの流れ：\n\n1. **選挙タイムアウト**発生 → Followerが**Candidate**に遷移\n2. **Term（任期）をインクリメント**\n3. **自分に投票** + 他ノードに**RequestVote RPC**を送信\n4. **過半数の票**を獲得 → **Leader**に遷移\n5. 他のLeaderを発見 → **Follower**に戻る\n\n<br />\n\n#### なぜランダムタイムアウトか\n\n選挙タイムアウトを150-300msの範囲でランダムに設定することで、複数ノードが同時にCandidateになる確率を下げています。これにより**Split Vote（票の分散）**を回避できます。\n\n```java\nprivate static final int ELECTION_TIMEOUT_MIN_MS = 150;\nprivate static final int ELECTION_TIMEOUT_MAX_MS = 300;\n\nprivate int getRandomElectionTimeout() {\n    return ThreadLocalRandom.current()\n        .nextInt(ELECTION_TIMEOUT_MIN_MS, ELECTION_TIMEOUT_MAX_MS + 1);\n}\n```\n\n<br />\n\n#### 選挙の実装\n\n```java\npublic void startElection() {\n    state = NodeState.CANDIDATE;\n    currentTerm++;\n    votedFor = nodeId;\n    int votes = 1; // 自分への投票\n\n    log.info(\"Node {} starting election for term {}\", nodeId, currentTerm);\n\n    // Virtual Threadsで並行にRequestVoteを送信\n    List<CompletableFuture<Boolean>> futures = peers.stream()\n        .map(peer -> CompletableFuture.supplyAsync(() -> {\n            try {\n                RequestVoteResponse response = sendRequestVote(peer,\n                    RequestVoteRequest.newBuilder()\n                        .setTerm(currentTerm)\n                        .setCandidateId(nodeId)\n                        .setLastLogIndex(log.getLastIndex())\n                        .setLastLogTerm(log.getLastTerm())\n                        .build()\n                );\n                return response.getVoteGranted();\n            } catch (Exception e) {\n                log.warn(\"Failed to get vote from {}\", peer, e);\n                return false;\n            }\n        }, executor))\n        .toList();\n\n    // 結果を集計\n    for (CompletableFuture<Boolean> future : futures) {\n        try {\n            if (future.get(ELECTION_TIMEOUT_MIN_MS, TimeUnit.MILLISECONDS)) {\n                votes++;\n            }\n        } catch (Exception e) {\n            // タイムアウトまたは失敗\n        }\n    }\n\n    // 過半数の票を獲得したらLeaderに\n    if (votes > (peers.size() + 1) / 2) {\n        becomeLeader();\n    }\n}\n\nprivate void becomeLeader() {\n    state = NodeState.LEADER;\n    log.info(\"Node {} became leader for term {}\", nodeId, currentTerm);\n\n    // 全Followerのnext/matchIndexを初期化\n    for (String peer : peers) {\n        nextIndex.put(peer, log.getLastIndex() + 1);\n        matchIndex.put(peer, 0);\n    }\n\n    // ハートビート送信開始\n    startHeartbeat();\n}\n```\n\n<br />\n\n### 3. ログレプリケーション (Log Replication)\n\nクライアントからの書き込み要求をログとして全ノードに複製する仕組みです。\n\n<br />\n\n#### レプリケーションの流れ\n\n```mermaid\nsequenceDiagram\n    participant C as Client\n    participant L as Leader\n    participant FA as Follower A\n    participant FB as Follower B\n\n    C->>L: PUT(key, val)\n    L->>FA: AppendEntries\n    L->>FB: AppendEntries\n    FA-->>L: Success\n    FB-->>L: Success\n    L->>C: OK (コミット完了)\n    L-->>FB: Heartbeat\n```\n\n<br />\n\n#### 実装コード\n\n```java\npublic class LogEntry {\n    private final int term;\n    private final int index;\n    private final Command command;\n\n    public LogEntry(int term, int index, Command command) {\n        this.term = term;\n        this.index = index;\n        this.command = command;\n    }\n}\n\npublic CompletableFuture<Boolean> appendEntry(Command command) {\n    if (state != NodeState.LEADER) {\n        return CompletableFuture.completedFuture(false);\n    }\n\n    // ログにエントリを追加\n    LogEntry entry = new LogEntry(currentTerm, log.getLastIndex() + 1, command);\n    log.append(entry);\n\n    // Virtual Threadsで並行にAppendEntriesを送信\n    AtomicInteger acks = new AtomicInteger(1); // 自分はすでにOK\n\n    List<CompletableFuture<Void>> futures = peers.stream()\n        .map(peer -> CompletableFuture.runAsync(() -> {\n            try {\n                AppendEntriesResponse response = sendAppendEntries(peer, entry);\n                if (response.getSuccess()) {\n                    acks.incrementAndGet();\n                    nextIndex.put(peer, entry.getIndex() + 1);\n                    matchIndex.put(peer, entry.getIndex());\n                } else {\n                    // ログの不整合 → nextIndexをデクリメントしてリトライ\n                    nextIndex.compute(peer, (k, v) -> Math.max(1, v - 1));\n                }\n            } catch (Exception e) {\n                log.warn(\"Failed to replicate to {}\", peer, e);\n            }\n        }, executor))\n        .toList();\n\n    // 結果を待機\n    return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))\n        .thenApply(v -> {\n            // 過半数がACKしたらコミット\n            if (acks.get() > (peers.size() + 1) / 2) {\n                commitIndex = entry.getIndex();\n                applyToStateMachine(entry);\n                return true;\n            }\n            return false;\n        });\n}\n```\n\n<br />\n\n### 4. gRPCによるノード間通信\n\nProtocol Buffersでメッセージを定義し、gRPCでRPCを実装しています。\n\n<br />\n\n#### Protocol Buffers定義\n\n```protobuf\nsyntax = \"proto3\";\n\npackage raft;\n\noption java_package = \"com.haroin57.raft.proto\";\noption java_multiple_files = true;\n\nservice RaftService {\n    // 投票要求\n    rpc RequestVote(RequestVoteRequest) returns (RequestVoteResponse);\n\n    // ログ追加（ハートビートも兼用）\n    rpc AppendEntries(AppendEntriesRequest) returns (AppendEntriesResponse);\n}\n\nmessage RequestVoteRequest {\n    int32 term = 1;\n    string candidate_id = 2;\n    int32 last_log_index = 3;\n    int32 last_log_term = 4;\n}\n\nmessage RequestVoteResponse {\n    int32 term = 1;\n    bool vote_granted = 2;\n}\n\nmessage LogEntry {\n    int32 term = 1;\n    int32 index = 2;\n    bytes command = 3;\n}\n\nmessage AppendEntriesRequest {\n    int32 term = 1;\n    string leader_id = 2;\n    int32 prev_log_index = 3;\n    int32 prev_log_term = 4;\n    repeated LogEntry entries = 5;\n    int32 leader_commit = 6;\n}\n\nmessage AppendEntriesResponse {\n    int32 term = 1;\n    bool success = 2;\n}\n```\n\n<br />\n\n#### gRPC サーバー実装\n\n```java\npublic class RaftServiceImpl extends RaftServiceGrpc.RaftServiceImplBase {\n    private final RaftNode node;\n\n    @Override\n    public void requestVote(RequestVoteRequest request,\n                           StreamObserver<RequestVoteResponse> responseObserver) {\n        boolean granted = node.handleRequestVote(\n            request.getTerm(),\n            request.getCandidateId(),\n            request.getLastLogIndex(),\n            request.getLastLogTerm()\n        );\n\n        responseObserver.onNext(RequestVoteResponse.newBuilder()\n            .setTerm(node.getCurrentTerm())\n            .setVoteGranted(granted)\n            .build());\n        responseObserver.onCompleted();\n    }\n\n    @Override\n    public void appendEntries(AppendEntriesRequest request,\n                             StreamObserver<AppendEntriesResponse> responseObserver) {\n        boolean success = node.handleAppendEntries(\n            request.getTerm(),\n            request.getLeaderId(),\n            request.getPrevLogIndex(),\n            request.getPrevLogTerm(),\n            request.getEntriesList(),\n            request.getLeaderCommit()\n        );\n\n        responseObserver.onNext(AppendEntriesResponse.newBuilder()\n            .setTerm(node.getCurrentTerm())\n            .setSuccess(success)\n            .build());\n        responseObserver.onCompleted();\n    }\n}\n```\n\n<br />\n\n### 5. 分散キーバリューストア\n\nRaftの上にシンプルなKVストアを構築しています。\n\n```java\npublic interface KeyValueStore {\n    CompletableFuture<Void> put(String key, String value);\n    String get(String key);\n    CompletableFuture<Void> delete(String key);\n}\n\npublic class RaftKeyValueStore implements KeyValueStore {\n    private final RaftNode raftNode;\n    private final ConcurrentHashMap<String, String> data = new ConcurrentHashMap<>();\n\n    @Override\n    public CompletableFuture<Void> put(String key, String value) {\n        Command cmd = new PutCommand(key, value);\n        return raftNode.appendEntry(cmd)\n            .thenAccept(success -> {\n                if (!success) {\n                    throw new NotLeaderException();\n                }\n            });\n    }\n\n    @Override\n    public String get(String key) {\n        // 読み取りはローカルから（Follower Readの場合）\n        // 強い一貫性が必要な場合はLeader経由\n        return data.get(key);\n    }\n\n    // ステートマシンへの適用\n    public void apply(Command command) {\n        if (command instanceof PutCommand put) {\n            data.put(put.getKey(), put.getValue());\n        } else if (command instanceof DeleteCommand delete) {\n            data.remove(delete.getKey());\n        }\n    }\n}\n```\n\n<br />\n\n### 6. 障害検知とフェイルオーバー\n\nノードの障害を検知して自動的にリーダーを再選出します。\n\n| パラメータ | 値 | 説明 |\n|-----------|-----|------|\n| Heartbeat間隔 | 50ms | Leaderが送信する間隔 |\n| 選挙タイムアウト | 150-300ms | ランダム化して衝突回避 |\n| RPC タイムアウト | 100ms | 通信タイムアウト |\n\n<br />\n\n## 学んだこと\n\n<br />\n\n### CAP定理の実感\n\n分散システムでは「一貫性（Consistency）」「可用性（Availability）」「分断耐性（Partition Tolerance）」のうち2つしか同時に満たせません。\n\nRaftは**CP（一貫性 + 分断耐性）**を選択しています：\n\n- 過半数のノードが利用不可 → 書き込み停止\n- データの一貫性は常に保証\n\n```mermaid\nflowchart TB\n    C[\"<strong>Consistency</strong><br/>一貫性\"]\n    A[\"Availability<br/>可用性\"]\n    P[\"<strong>Partition Tolerance</strong><br/>分断耐性\"]\n\n    C ---|\"Raft\"| P\n    C -.-|\"×\"| A\n    A -.-|\"×\"| P\n\n    style C fill:#134e4a,stroke:#5eead4,stroke-width:2px\n    style P fill:#134e4a,stroke:#5eead4,stroke-width:2px\n```\n\n<br />\n\n### 分散システム特有の難しさ\n\n| 課題 | 説明 |\n|------|------|\n| **ネットワーク遅延** | メッセージの到着順序が保証されない |\n| **ネットワーク分断** | 一部のノードと通信不能になる |\n| **クロックの同期** | 各ノードの時計がずれる |\n| **障害の種類** | クラッシュ、ビザンチン障害など |\n| **再試行の冪等性** | 同じ操作を複数回実行しても結果が変わらない設計 |\n\n<br />\n\n### デバッグの大変さ\n\n複数ノードが非同期に動作するため、バグの再現が困難です。\n\n対策として実施したこと：\n\n1. **詳細なログ出力** - イベントの順序を追跡\n2. **論理クロック** - 因果関係を把握\n3. **決定論的テスト** - 乱数シードを固定\n\n```java\n// イベントログの例\nlog.info(\"[Term={}][Node={}][State={}] {}\",\n    currentTerm, nodeId, state, \"Received AppendEntries from \" + leaderId);\n```\n\n<br />\n\n## テスト\n\n<br />\n\n### 単体テスト\n\n各コンポーネントを個別にテスト：\n\n```java\n@Test\nvoid shouldGrantVoteIfTermIsHigher() {\n    RaftNode node = new RaftNode(\"node1\", List.of(\"node2\", \"node3\"));\n\n    boolean granted = node.handleRequestVote(\n        /* term */ 2,\n        /* candidateId */ \"node2\",\n        /* lastLogIndex */ 0,\n        /* lastLogTerm */ 0\n    );\n\n    assertThat(granted).isTrue();\n    assertThat(node.getVotedFor()).isEqualTo(\"node2\");\n}\n```\n\n<br />\n\n### 統合テスト\n\n複数ノードを起動して実際の動作を確認：\n\n```java\n@Test\nvoid shouldElectLeaderIn3NodeCluster() throws Exception {\n    // 3ノードのクラスタを起動\n    List<RaftNode> nodes = startCluster(3);\n\n    // リーダーが選出されるまで待機\n    await().atMost(Duration.ofSeconds(5))\n        .until(() -> countLeaders(nodes) == 1);\n\n    // 1つだけリーダーがいることを確認\n    assertThat(countLeaders(nodes)).isEqualTo(1);\n}\n\n@Test\nvoid shouldReelectLeaderWhenLeaderFails() throws Exception {\n    List<RaftNode> nodes = startCluster(3);\n    await().until(() -> countLeaders(nodes) == 1);\n\n    // リーダーを停止\n    RaftNode leader = findLeader(nodes);\n    leader.shutdown();\n\n    // 新しいリーダーが選出されるまで待機\n    await().atMost(Duration.ofSeconds(5))\n        .until(() -> countLeaders(nodes) == 1);\n}\n```\n\n<br />\n\n## 参考資料\n\n- [In Search of an Understandable Consensus Algorithm (Raft論文)](https://raft.github.io/raft.pdf)\n- [The Raft Consensus Algorithm](https://raft.github.io/)\n- [Raft Visualization](http://thesecretlivesofdata.com/raft/)\n- [Designing Data-Intensive Applications](https://dataintensive.net/)\n\n<br />\n\n## 今後の改善予定\n\n- [ ] ログの永続化（ファイル保存）\n- [ ] 簡易的なCLIクライアントの作成\n- [ ] ノードの状態をWebで可視化\n- [ ] テストカバレッジの向上\n- [ ] READMEの充実\n\n<br />\n\n## まとめ\n\n分散システムは理論を読むだけでは理解が難しく、実際に実装してみることで多くの気づきがありました。\n\n特に以下の点は、実装して初めて腑に落ちました：\n\n- リーダー選出で**なぜランダムなタイムアウトが必要か**\n- ログレプリケーションで**なぜ過半数で良いのか**\n- **ネットワーク分断時にどう振る舞うか**\n\nRaftは「理解しやすい」を目指したアルゴリズムですが、それでも実装には多くの edge case があり、分散システムの難しさを実感しました。\n\n<br />\n\nソースコード: [haroin57/distributed-system](https://github.com/haroin57/distoributed-system)\n","html":""}